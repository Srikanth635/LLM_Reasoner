{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Agents are connected but have their own states. Only the final responses are appended to global state\n",
    "\n",
    "<img src=\"../../resources/images/multi_agent_supervisor.png\" alt=\"Multi Agent Supervisor\" height=\"300\" width=\"300\">\n",
    "\n",
    "Agents are independent LangChain agents. This means they have their own individual prompt, LLM, and tools."
   ],
   "id": "bd92306529b510c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T13:54:39.866167Z",
     "start_time": "2025-05-13T13:54:39.305406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "from src.langchain.lg_workflow import *"
   ],
   "id": "3442cb04371686b8",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T13:54:42.767303Z",
     "start_time": "2025-05-13T13:54:40.629382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.langchain.agents.framenet_agent import *\n",
    "fnr = framenet_tool.invoke({'instruction':'pick up the cup'})\n",
    "fnr"
   ],
   "id": "3bd3077ecf3cf38a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSIDE FRAMENET TOOL\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\\n  \"framenet\": \"picking_up\",\\n  \"frame\": \"Getting\",\\n  \"lexical-unit\": \"pick up.v\",\\n  \"core\": {\\n    \"agent\": \"robot\",\\n    \"theme_patient\": \"cup\",\\n    \"instrument\": \"robot gripper\",\\n    \"source\": \"surface\",\\n    \"goal\": \"cup\",\\n    \"result\": \"robot has the cup\"\\n  },\\n  \"peripheral\": {\\n    \"location\": \"workspace\",\\n    \"manner\": \"gently\",\\n    \"direction\": \"upward\",\\n    \"time\": \"during operation\",\\n    \"quantity\": \"one item\",\\n    \"portion\": \"\"\\n  }\\n}'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T13:54:57.378844Z",
     "start_time": "2025-05-13T13:54:44.508542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example: Complex Query Using Multiple Agents\n",
    "input_question = \"framenet representation of the action pick up the mug from the sink\"\n",
    "for s in graph.stream(\n",
    "    {\"messages\": [(\"user\", input_question)]},\n",
    "    subgraphs=True\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")\n"
   ],
   "id": "d1d826be26eb7678",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next Worker: framenet\n",
      "((), {'supervisor': None})\n",
      "----\n",
      "INSIDE FRAMENET TOOL\n",
      "(('framenet:4e82edc6-c0e3-8b96-d4f6-309e197939d9',), {'agent': {'messages': [AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2025-05-13T13:54:50.06727488Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4926080106, 'load_duration': 8142009, 'prompt_eval_count': 294, 'prompt_eval_duration': 40061265, 'eval_count': 352, 'eval_duration': 4871349509, 'model_name': 'qwen3:8b'}, id='run--238ded2f-a2bf-4642-9fbe-46f9b271adec-0', tool_calls=[{'name': 'framenet_tool', 'args': {'instruction': 'pick up the mug from the sink'}, 'id': '5f885a89-5ad9-4db2-bf24-9301530a88ee', 'type': 'tool_call'}], usage_metadata={'input_tokens': 294, 'output_tokens': 352, 'total_tokens': 646})]}})\n",
      "----\n",
      "(('framenet:4e82edc6-c0e3-8b96-d4f6-309e197939d9',), {'tools': {'messages': [ToolMessage(content='{\\n  \"framenet\": \"picking_up\",\\n  \"frame\": \"Getting\",\\n  \"lexical-unit\": \"pick up.v\",\\n  \"core\": {\\n    \"agent\": \"robot\",\\n    \"theme_patient\": \"mug\",\\n    \"instrument\": \"robot gripper\",\\n    \"source\": \"sink\",\\n    \"goal\": \"\",\\n    \"result\": \"robot has the mug\"\\n  },\\n  \"peripheral\": {\\n    \"location\": \"kitchen workspace\",\\n    \"manner\": \"gently\",\\n    \"direction\": \"upward\",\\n    \"time\": \"during cleanup phase\",\\n    \"quantity\": \"one item\",\\n    \"portion\": \"\"\\n  }\\n}', name='framenet_tool', id='2a77565f-b535-4c61-bbe1-1a30adbdec18', tool_call_id='5f885a89-5ad9-4db2-bf24-9301530a88ee')]}})\n",
      "----\n",
      "(('framenet:4e82edc6-c0e3-8b96-d4f6-309e197939d9',), {'agent': {'messages': [AIMessage(content='<think>\\nOkay, let me break this down. The user asked for a FrameNet representation of the action \"pick up the mug from the sink.\" I called the framenet_tool with that instruction. The response came back with a structured output.\\n\\nFirst, I need to present the FrameNet representation as specified. The main elements are the frame \"Getting\" with the lexical unit \"pick up.v\". The core elements include agent (robot), theme_patient (mug), instrument (robot gripper), source (sink), and the result. Peripheral details like location, manner, etc., are also there.\\n\\nI should format this in a clear, organized way. Maybe start with the frame and lexical unit, then list the core elements, followed by peripheral. Use bullet points or sections for readability. Make sure to mention each part as per the tool\\'s output without adding extra info. Keep it straightforward since the user probably wants a direct answer without explanations.\\n</think>\\n\\nHere is the FrameNet representation for the action \"pick up the mug from the sink\":\\n\\n```\\nFrame: Getting\\nLexical Unit: pick up.v\\nCore Elements:\\n  - agent: robot\\n  - theme_patient: mug\\n  - instrument: robot gripper\\n  - source: sink\\n  - result: robot has the mug\\n\\nPeripheral Elements:\\n  - location: kitchen workspace\\n  - manner: gently\\n  - direction: upward\\n  - time: during cleanup phase\\n  - quantity: one item\\n```', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2025-05-13T13:54:56.604460047Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4305321361, 'load_duration': 8747143, 'prompt_eval_count': 473, 'prompt_eval_duration': 54552714, 'eval_count': 303, 'eval_duration': 4230663600, 'model_name': 'qwen3:8b'}, id='run--f357d78e-7899-4dcb-84c0-b01475084fa5-0', usage_metadata={'input_tokens': 473, 'output_tokens': 303, 'total_tokens': 776})]}})\n",
      "----\n",
      "((), {'framenet': {'messages': [HumanMessage(content='<think>\\nOkay, let me break this down. The user asked for a FrameNet representation of the action \"pick up the mug from the sink.\" I called the framenet_tool with that instruction. The response came back with a structured output.\\n\\nFirst, I need to present the FrameNet representation as specified. The main elements are the frame \"Getting\" with the lexical unit \"pick up.v\". The core elements include agent (robot), theme_patient (mug), instrument (robot gripper), source (sink), and the result. Peripheral details like location, manner, etc., are also there.\\n\\nI should format this in a clear, organized way. Maybe start with the frame and lexical unit, then list the core elements, followed by peripheral. Use bullet points or sections for readability. Make sure to mention each part as per the tool\\'s output without adding extra info. Keep it straightforward since the user probably wants a direct answer without explanations.\\n</think>\\n\\nHere is the FrameNet representation for the action \"pick up the mug from the sink\":\\n\\n```\\nFrame: Getting\\nLexical Unit: pick up.v\\nCore Elements:\\n  - agent: robot\\n  - theme_patient: mug\\n  - instrument: robot gripper\\n  - source: sink\\n  - result: robot has the mug\\n\\nPeripheral Elements:\\n  - location: kitchen workspace\\n  - manner: gently\\n  - direction: upward\\n  - time: during cleanup phase\\n  - quantity: one item\\n```', additional_kwargs={}, response_metadata={}, name='framenet')]}})\n",
      "----\n",
      "Next Worker: FINISH\n",
      "((), {'supervisor': None})\n",
      "----\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T14:06:41.682174Z",
     "start_time": "2025-05-13T14:06:28.326607Z"
    }
   },
   "cell_type": "code",
   "source": "graph.invoke({'messages': [HumanMessage(content='framenet representation of the action pick up the mug from the fridge')]})['messages'][0].content",
   "id": "b827a45b1d8ad47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next Worker: framenet\n",
      "INSIDE FRAMENET TOOL\n",
      "Next Worker: FINISH\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'framenet representation of the action pick up the mug from the fridge'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T14:07:01.087586Z",
     "start_time": "2025-05-13T14:06:44.972079Z"
    }
   },
   "cell_type": "code",
   "source": "graph.invoke({'messages': [HumanMessage(content='framenet representation of the action pick up the mug from the fridge')]})['messages'][1].content",
   "id": "5547cddfa73627e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next Worker: framenet\n",
      "INSIDE FRAMENET TOOL\n",
      "Next Worker: FINISH\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, the user asked for the FrameNet representation of \"pick up the mug from the fridge.\" I called the framenet_tool with that instruction. The response came back with the frame \"Getting\" and the lexical unit \"pick up.v\". The core elements have agent as robot, theme_patient as mug, instrument as robot gripper, source as fridge, and the result is the robot having the mug. Peripheral details include location, manner, direction, etc.\\n\\nNow, I need to present this information as the final answer. The user probably wants a clear, structured output without extra explanations. Let me format the response using the FrameNet elements provided. Make sure to include all the core and peripheral elements as listed. Use the exact terms from the tool\\'s response. Avoid any markdown and just present the data in a readable way. Check if all the keys and values are correctly transcribed. Alright, that should do it.\\n</think>\\n\\n{\\n  \"framenet\": \"picking_up\",\\n  \"frame\": \"Getting\",\\n  \"lexical-unit\": \"pick up.v\",\\n  \"core\": {\\n    \"agent\": \"robot\",\\n    \"theme_patient\": \"mug\",\\n    \"instrument\": \"robot gripper\",\\n    \"source\": \"fridge\",\\n    \"goal\": \"\",\\n    \"result\": \"robot has the mug\"\\n  },\\n  \"peripheral\": {\\n    \"location\": \"kitchen\",\\n    \"manner\": \"carefully\",\\n    \"direction\": \"upward\",\\n    \"time\": \"during retrieval\",\\n    \"quantity\": \"one item\",\\n    \"portion\": \"\"\\n  }\\n}'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T14:17:13.464517Z",
     "start_time": "2025-05-13T14:16:59.710900Z"
    }
   },
   "cell_type": "code",
   "source": "framenet_agent.invoke({'messages': [HumanMessage(content='pick up the mug from the sink')]})",
   "id": "1b4fdcbaed17b4e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSIDE FRAMENET TOOL\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='pick up the mug from the sink', additional_kwargs={}, response_metadata={}, id='79595347-d922-4667-bfee-635f79170e8a'),\n",
       "  AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2025-05-13T14:17:04.579819389Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4861643388, 'load_duration': 1256268372, 'prompt_eval_count': 259, 'prompt_eval_duration': 160543146, 'eval_count': 260, 'eval_duration': 3443111394, 'model_name': 'qwen3:8b'}, id='run--8c46cadf-7a18-4960-a4ff-da533aaf85a2-0', tool_calls=[{'name': 'framenet_tool', 'args': {'instruction': 'pick up the mug from the sink'}, 'id': 'f31c17c5-eb6e-4a92-8dc5-2603a5bf3863', 'type': 'tool_call'}], usage_metadata={'input_tokens': 259, 'output_tokens': 260, 'total_tokens': 519}),\n",
       "  ToolMessage(content='{\\n  \"framenet\": \"picking_up\",\\n  \"frame\": \"Getting\",\\n  \"lexical-unit\": \"pick up.v\",\\n  \"core\": {\\n    \"agent\": \"robot\",\\n    \"theme_patient\": \"mug\",\\n    \"instrument\": \"robot gripper\",\\n    \"source\": \"sink\",\\n    \"goal\": \"\",\\n    \"result\": \"robot has the mug\"\\n  },\\n  \"peripheral\": {\\n    \"location\": \"kitchen workspace\",\\n    \"manner\": \"gently\",\\n    \"direction\": \"upward\",\\n    \"time\": \"during cleanup phase\",\\n    \"quantity\": \"one item\",\\n    \"portion\": \"\"\\n  }\\n}', name='framenet_tool', id='d72fbbcd-8207-408b-a3f2-03269c86328f', tool_call_id='f31c17c5-eb6e-4a92-8dc5-2603a5bf3863'),\n",
       "  AIMessage(content='<think>\\nOkay, let\\'s see. The user asked the robot to \"pick up the mug from the sink.\" I used the FrameNet tool to break down the semantics of that instruction. The tool returned a detailed frame with core and peripheral elements.\\n\\nFirst, the core part has the agent as the robot, which makes sense because the robot is performing the action. The theme_patient is the mug, so that\\'s the object being picked up. The instrument mentioned is the robot gripper, which is the tool used to pick up the mug. The source is the sink, indicating where the mug is located. The goal and result fields are empty except for the result stating the robot has the mug, which is logical.\\n\\nLooking at the peripheral elements, the location is the kitchen workspace, which might be inferred from the context of a sink. The manner is \"gently,\" which wasn\\'t explicitly mentioned but could be a default assumption. Direction is upward, which aligns with picking something up. Time is during cleanup phase, again a possible contextual assumption. Quantity is one item, matching the mug. Portion is empty, so maybe that\\'s not relevant here.\\n\\nI need to present this information clearly. The user probably wants to know the breakdown of the action for understanding or system verification. They might be testing if the tool correctly identifies the components. I should highlight the key elements: agent, patient, instrument, source, and the result. Also, note any inferred details like location and manner, but mention that they might be assumptions. Make sure to keep it concise and structured, maybe using bullet points for clarity.\\n</think>\\n\\nHere\\'s the semantic breakdown of the instruction \"pick up the mug from the sink\":\\n\\n**Core Semantic Roles:**\\n- **Agent**: Robot (performer of the action)\\n- **Theme/Patient**: Mug (object being picked up)\\n- **Instrument**: Robot gripper (tool used)\\n- **Source**: Sink (location of the object)\\n- **Result**: Robot has the mug (outcome of the action)\\n\\n**Peripheral Context:**\\n- **Location**: Kitchen workspace (implied context)\\n- **Manner**: Gently (assumed default)\\n- **Direction**: Upward (implicit in \"pick up\")\\n- **Time**: During cleanup phase (contextual assumption)\\n- **Quantity**: One item (matches the mug)\\n\\nThis FrameNet representation captures both explicit elements from the instruction and reasonable contextual inferences.', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2025-05-13T14:17:13.461337548Z', 'done': True, 'done_reason': 'stop', 'total_duration': 6763878729, 'load_duration': 8559796, 'prompt_eval_count': 438, 'prompt_eval_duration': 48085386, 'eval_count': 492, 'eval_duration': 6699750867, 'model_name': 'qwen3:8b'}, id='run--06d58fc0-9bcd-4845-90c6-bbdbe5fe018a-0', usage_metadata={'input_tokens': 438, 'output_tokens': 492, 'total_tokens': 930})]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from IPython.display import display, Image\n",
    "# display(Image(math_agent.get_graph().draw_mermaid_png()))\n",
    "# display(Image(websearch_agent.get_graph().draw_mermaid_png()))\n",
    "# display(Image(framenet_agent.get_graph().draw_mermaid_png()))\n",
    "# display(Image(graph.get_graph().draw_mermaid_png()))"
   ],
   "id": "6fb01b822c99559e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example: Complex Query Using Multiple Agents\n",
    "input_question = \"what is 2 times 2\"\n",
    "for s in graph.stream(\n",
    "    {\"messages\": [(\"user\", input_question)]},\n",
    "    subgraphs=True\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")\n"
   ],
   "id": "e54ed6f87aad0a36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example: Complex Query Using Multiple Agents\n",
    "input_question = \"what is the value of 2 + 10\"\n",
    "for s in graph.stream(\n",
    "    {\"messages\": [(\"user\", input_question)]},\n",
    "    subgraphs=True\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")\n"
   ],
   "id": "fc9e1a5c75b55215",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Example: Complex Query Using Multiple Agents\n",
    "# input_question = \"who is apple company founder\"\n",
    "# for s in graph.stream(\n",
    "#     {\"messages\": [(\"user\", input_question)]},\n",
    "#     subgraphs=True\n",
    "# ):\n",
    "#     print(s)\n",
    "#     print(\"----\")\n"
   ],
   "id": "b9980ce4b212f44a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f631ea35b1ae8d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:31:59.477932Z",
     "start_time": "2025-05-13T11:31:59.476069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Example: Complex Query Using Multiple Agents\n",
    "# input_question = \"who is the ceo of google and what is the framenet representation of apple\"\n",
    "# for s in graph.stream(\n",
    "#     {\"messages\": [(\"user\", input_question)]},\n",
    "#     subgraphs=True\n",
    "# ):\n",
    "#     print(s)\n",
    "#     print(\"----\")\n"
   ],
   "id": "7eb90ed0041629cc",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:32:02.295127Z",
     "start_time": "2025-05-13T11:32:02.293584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Example: Complex Query Using Multiple Agents\n",
    "# input_question = \"who is the ceo of google, what is the framenet representation of apple and how much is 2 times 10\"\n",
    "# for s in graph.stream(\n",
    "#     {\"messages\": [(\"user\", input_question)]},\n",
    "#     subgraphs=True\n",
    "# ):\n",
    "#     print(s)\n",
    "#     print(\"----\")\n"
   ],
   "id": "7274ab3752af7aaf",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3481b4af5579e10f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
