{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Agents are connected but have their own states. Only the final responses are appended to global state\n",
    "\n",
    "<img src=\"../../resources/images/multi_agent_supervisor.png\" alt=\"Multi Agent Supervisor\" height=\"300\" width=\"300\">\n",
    "\n",
    "Agents are independent LangChain agents. This means they have their own individual prompt, LLM, and tools."
   ],
   "id": "bd92306529b510c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T14:35:27.097307Z",
     "start_time": "2025-05-21T14:35:26.223784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "from src.langchain.lg_workflow import *\n",
    "from src.langchain.agents.framenet_agent import *\n",
    "from src.langchain.agents.flanagan_agent import *\n",
    "from src.langchain.agents.pycram_agent import *\n",
    "config = {\"configurable\" : {\"thread_id\" : 1}}"
   ],
   "id": "3442cb04371686b8",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Framenet Agent",
   "id": "37d0c51a4f0fbdf9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fnr = framenet_tool.invoke({'instruction':'pick up the cup'})\n",
    "fnr"
   ],
   "id": "3bd3077ecf3cf38a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example: Complex Query Using Multiple Agents\n",
    "input_question = \"what is 2 plus 2\"\n",
    "for s in graph.stream(\n",
    "    {\"messages\": [(\"user\", input_question)]},\n",
    "    subgraphs=True,\n",
    "    config=config,\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")\n"
   ],
   "id": "d1d826be26eb7678",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(graph.get_state(config))",
   "id": "5fe2bd5f0a565fd8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example: Complex Query Using Multiple Agents\n",
    "input_question = \"framenet representation of the action instruction pour water from the bottle into the container.\"\n",
    "for s in graph.stream(\n",
    "    {\"messages\": [(\"user\", input_question)]},\n",
    "    subgraphs=True,\n",
    "    config=config,\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")\n"
   ],
   "id": "5c21296a78044bb2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "graph.get_state(config)",
   "id": "bd7982a94a5903db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "graph.invoke({'messages': [HumanMessage(content='framenet representation of the action pick up the cup from the fridge')]}, config=config)['messages'][0].content",
   "id": "9dffdd1545ffc558",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "graph.get_state(config=config)",
   "id": "b827a45b1d8ad47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# graph.get_state(config=config).values['messages'][-1].content\n",
    "framenet_answers"
   ],
   "id": "accb1841acaf3fa2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "graph.invoke({'messages': [HumanMessage(content='framenet representation of the action pick up the mug from the fridge')]})['messages'][1].content",
   "id": "5547cddfa73627e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Flanagan Agent",
   "id": "5962017aee5d2c96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example: Complex Query Using Multiple Agents\n",
    "input_question = \"flanagan representation of the action instruction pour water from the bottle into the container.\"\n",
    "for s in graph.stream(\n",
    "    {\"messages\": [(\"user\", input_question)]},\n",
    "    subgraphs=True,\n",
    "    config=config,\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")\n"
   ],
   "id": "1b4fdcbaed17b4e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "graph.invoke({'messages': [HumanMessage(content='flanagan representation of instruction pickup the bottle from the fridge')]}, config=config)['messages'][1].content",
   "id": "de612399a406c7e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "flanagan_agent.invoke({'messages': [HumanMessage(content='flanagan representation of instruction pour water from the bottle into the container')]})",
   "id": "589aeddfabda34d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from IPython.display import display, Image\n",
    "# display(Image(math_agent.get_graph().draw_mermaid_png()))\n",
    "# display(Image(websearch_agent.get_graph().draw_mermaid_png()))\n",
    "# display(Image(framenet_agent.get_graph().draw_mermaid_png()))\n",
    "# display(Image(graph.get_graph().draw_mermaid_png()))"
   ],
   "id": "6fb01b822c99559e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Pycram Agent",
   "id": "f755f645eb04a0bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T14:36:10.911993Z",
     "start_time": "2025-05-21T14:35:41.930714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example: Complex Query Using Multiple Agents\n",
    "input_question = \"pick up the cup and go to sink\"\n",
    "for s in pycram_agent.stream(\n",
    "    {\"messages\": [(\"user\", input_question)]},\n",
    "    subgraphs=True,\n",
    "    config=config,\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")\n"
   ],
   "id": "14f94ce6581cbf91",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((), {'agent': {'messages': [AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2025-05-21T14:35:46.065829805Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4126365133, 'load_duration': 13306758, 'prompt_eval_count': 761, 'prompt_eval_duration': 41286147, 'eval_count': 287, 'eval_duration': 4065502783, 'model_name': 'qwen3:8b'}, id='run--12acfdfd-6731-4976-aa1f-e87e0da74378-0', tool_calls=[{'name': 'model_selector', 'args': {'instruction': 'pick up the cup and go to sink'}, 'id': 'c5dfd5c5-cde7-4128-91d6-1837f24f879a', 'type': 'tool_call'}], usage_metadata={'input_tokens': 761, 'output_tokens': 287, 'total_tokens': 1048})]}})\n",
      "----\n",
      "INSIDE MODEL SELECTOR TOOL\n",
      "response of tool 1 :  <class 'src.langchain.agents.pycram_agent.ActionNames'> model_names=['PickUpAction', 'NavigateAction']\n",
      "((), {'tools': {'messages': [ToolMessage(content='{\"model_names\": [\"PickUpAction\", \"NavigateAction\"]}', name='model_selector', tool_call_id='c5dfd5c5-cde7-4128-91d6-1837f24f879a')]}})\n",
      "----\n",
      "((), {'agent': {'messages': [AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2025-05-21T14:35:55.025092661Z', 'done': True, 'done_reason': 'stop', 'total_duration': 8498334369, 'load_duration': 11875632, 'prompt_eval_count': 812, 'prompt_eval_duration': 182860832, 'eval_count': 597, 'eval_duration': 8290368268, 'model_name': 'qwen3:8b'}, id='run--8e00c2fb-627a-4532-9cc5-998106d4a4b8-0', tool_calls=[{'name': 'model_populator', 'args': {'instruction': 'pick up the cup and go to sink', 'model_names': ['PickUpAction', 'NavigateAction']}, 'id': '97a356d7-07db-45ae-954b-8580ef1caae8', 'type': 'tool_call'}], usage_metadata={'input_tokens': 812, 'output_tokens': 597, 'total_tokens': 1409})]}})\n",
      "----\n",
      "INSIDE MODEL POPULATOR TOOL\n",
      "((), {'tools': {'messages': [ToolMessage(content='{\"models\": [{\"robot_position\": {\"position\": [1.2, 0.5], \"orientation\": [0.0, 0.0, 0.0]}, \"robot_torso_height\": 1.0, \"robot_type\": null, \"action_type\": \"PickUpAction\", \"object_designator\": {\"name\": \"cup\", \"concept\": \"cup\", \"path\": null, \"pose\": null, \"world\": null, \"color\": null, \"ignore_cached_files\": false, \"scale_mesh\": null, \"mesh_transform\": null}, \"arm\": 1, \"grasp_description\": {\"approach_direction\": \"front\", \"vertical_alignment\": \"top\", \"rotate_gripper\": true}}, {\"robot_position\": {\"position\": [1.2, 0.5], \"orientation\": [0.0, 0.0, 0.0]}, \"robot_torso_height\": 1.0, \"robot_type\": null, \"action_type\": \"NavigateAction\", \"target_location\": {\"position\": [1.2, 0.5], \"orientation\": [0.0, 0.0, 0.0]}, \"keep_joint_states\": true}]}', name='model_populator', tool_call_id='97a356d7-07db-45ae-954b-8580ef1caae8')]}})\n",
      "----\n",
      "((), {'agent': {'messages': [AIMessage(content='<think>\\nOkay, let\\'s see. The user asked the robot to \"pick up the cup and go to sink\". First, I need to break down the instruction into individual actions. The model_selector tool was used and returned [\"PickUpAction\", \"NavigateAction\"], which makes sense because picking up an object and navigating to a location are two separate actions.\\n\\nNow, the model_populator tool has generated the structured models. Let me check the output. For the NavigateAction, the target_location is set to the same position as the robot\\'s current position. That doesn\\'t seem right because the user mentioned going to the sink. Maybe the sink\\'s position wasn\\'t specified in the original instruction, so the tool defaulted to the current position. But that\\'s not helpful. Wait, perhaps the sink\\'s location is assumed to be a known place in the environment. If the robot is already near the sink, maybe the target is correct. However, if the sink is elsewhere, the model might be incomplete.\\n\\nFor the PickUpAction, the object_designator has the name \"cup\", which matches the user\\'s request. The arm is set to 1 (probably left arm), and the grasp_description has approach_direction as front, which is reasonable. But the target location for the NavigateAction is the same as the current position. That might be an error because the robot needs to move to the sink first before picking up the cup. Wait, the original instruction says \"go to sink\" first, then pick up the cup. So the NavigateAction should be to the sink\\'s location, and then the PickUpAction would be at the sink. However, in the model_populator output, the NavigateAction\\'s target is the same as the robot\\'s current position. That suggests the sink\\'s position wasn\\'t specified, so the tool couldn\\'t determine it. Maybe the system assumes the sink is at a default location, but the user didn\\'t mention it. \\n\\nI need to make sure the actions are in the correct order: navigate to the sink first, then pick up the cup. The current models have both actions, but the NavigateAction isn\\'t moving. Maybe the tool used default values. Since the user didn\\'t specify the sink\\'s coordinates, the model might be incomplete. But according to the tool\\'s response, it\\'s generating the models as best as possible. So the final answer should present these models, even if some parameters are default. The user might need to provide more details, but based on the given info, this is the structured output.\\n</think>\\n\\nThe robot will first navigate to the sink (though the target location is unspecified and defaults to current position) and then pick up the cup. Here are the structured actions:\\n\\n```json\\n[\\n  {\\n    \"action_type\": \"NavigateAction\",\\n    \"target_location\": {\\n      \"position\": [1.2, 0.5],\\n      \"orientation\": [0.0, 0.0, 0.0, 1.0]\\n    },\\n    \"keep_joint_states\": true\\n  },\\n  {\\n    \"action_type\": \"PickUpAction\",\\n    \"object_designator\": {\\n      \"name\": \"cup\",\\n      \"concept\": \"cup\"\\n    },\\n    \"arm\": \"left\",\\n    \"grasp_description\": {\\n      \"approach_direction\": \"front\",\\n      \"vertical_alignment\": \"top\",\\n      \"rotate_gripper\": true\\n    }\\n  }\\n]\\n```\\n\\nNote: The navigation target location is unspecified in the original instruction and defaults to the robot\\'s current position. The cup\\'s exact location and orientation are also unspecified.', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2025-05-21T14:36:10.908399465Z', 'done': True, 'done_reason': 'stop', 'total_duration': 10684605585, 'load_duration': 9129593, 'prompt_eval_count': 1120, 'prompt_eval_duration': 351305517, 'eval_count': 734, 'eval_duration': 10296521375, 'model_name': 'qwen3:8b'}, id='run--769c41ad-3682-4795-8c39-196634584e99-0', usage_metadata={'input_tokens': 1120, 'output_tokens': 734, 'total_tokens': 1854})]}})\n",
      "----\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T14:37:42.353188Z",
     "start_time": "2025-05-21T14:37:04.041156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pycram_agent.invoke({\"messages\" : [HumanMessage(content=\"pick up the cup from the sink nad place it on the table\")]})\n",
    "# pycram_agent.invoke({\"messages\": \"pick up the cup from the sink\"})"
   ],
   "id": "f82bc2b0adb16954",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSIDE MODEL SELECTOR TOOL\n",
      "response of tool 1 :  <class 'src.langchain.agents.pycram_agent.ActionNames'> model_names=['PickUpAction', 'NavigateAction', 'PlaceAction']\n",
      "INSIDE MODEL POPULATOR TOOL\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='pick up the cup from the sink nad place it on the table', additional_kwargs={}, response_metadata={}, id='d2d893bf-1067-4b4a-a45c-ea3a0057d81c'),\n",
       "  AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2025-05-21T14:37:07.905881324Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3858742822, 'load_duration': 15010790, 'prompt_eval_count': 766, 'prompt_eval_duration': 19129209, 'eval_count': 269, 'eval_duration': 3819956112, 'model_name': 'qwen3:8b'}, id='run--b6fb78e7-7957-4984-beb0-114f625fff31-0', tool_calls=[{'name': 'model_selector', 'args': {'instruction': 'pick up the cup from the sink nad place it on the table'}, 'id': '6a8ef7af-4656-4c59-bec1-fc69f9d2df0b', 'type': 'tool_call'}], usage_metadata={'input_tokens': 766, 'output_tokens': 269, 'total_tokens': 1035}),\n",
       "  ToolMessage(content='{\"model_names\": [\"PickUpAction\", \"NavigateAction\", \"PlaceAction\"]}', name='model_selector', id='a8d35b2a-ab56-46f3-a629-b7b878c50515', tool_call_id='6a8ef7af-4656-4c59-bec1-fc69f9d2df0b'),\n",
       "  AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2025-05-21T14:37:14.429030381Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5894433045, 'load_duration': 13514435, 'prompt_eval_count': 826, 'prompt_eval_duration': 182803783, 'eval_count': 372, 'eval_duration': 5683392362, 'model_name': 'qwen3:8b'}, id='run--97143d92-f287-4edd-943a-0c0307be4764-0', tool_calls=[{'name': 'model_populator', 'args': {'instruction': 'pick up the cup from the sink nad place it on the table', 'model_names': ['PickUpAction', 'NavigateAction', 'PlaceAction']}, 'id': '2fb4be9b-97cd-46f4-afab-670aede64ba5', 'type': 'tool_call'}], usage_metadata={'input_tokens': 826, 'output_tokens': 372, 'total_tokens': 1198}),\n",
       "  ToolMessage(content='{\"models\": [{\"robot_position\": {\"position\": [-0.5, 0.0, 0.0], \"orientation\": [0.0, 0.0, 0.0, 1.0]}, \"robot_torso_height\": 1.5, \"robot_type\": null, \"action_type\": \"PickUpAction\", \"object_designator\": {\"name\": \"cup\", \"concept\": \"cup\", \"path\": null, \"pose\": null, \"world\": null, \"color\": null, \"ignore_cached_files\": false, \"scale_mesh\": null, \"mesh_transform\": null}, \"arm\": 0, \"grasp_description\": {\"approach_direction\": \"front\", \"vertical_alignment\": \"top\", \"rotate_gripper\": true}}, {\"robot_position\": {\"position\": [-0.5, 0.0, 0.0], \"orientation\": [0.0, 0.0, 0.0, 1.0]}, \"robot_torso_height\": 1.5, \"robot_type\": null, \"action_type\": \"NavigateAction\", \"target_location\": {\"position\": [1.2, 0.5, 0.0], \"orientation\": [0.0, 0.0, 0.0, 1.0]}, \"keep_joint_states\": null}, {\"robot_position\": {\"position\": [-0.5, 0.0, 0.0], \"orientation\": [0.0, 0.0, 0.0, 1.0]}, \"robot_torso_height\": 1.5, \"robot_type\": null, \"action_type\": \"PlaceAction\", \"object_designator\": {\"name\": \"cup\", \"concept\": \"cup\", \"path\": null, \"pose\": null, \"world\": null, \"color\": null, \"ignore_cached_files\": false, \"scale_mesh\": null, \"mesh_transform\": null}, \"target_location\": {\"position\": [1.2, 0.5, 0.0], \"orientation\": [0.0, 0.0, 0.0, 1.0]}, \"arm\": 0}]}', name='model_populator', id='b7c2bf1a-21da-490f-999d-67350e329daa', tool_call_id='2fb4be9b-97cd-46f4-afab-670aede64ba5'),\n",
       "  AIMessage(content='<think>\\nOkay, let\\'s see. The user asked to pick up the cup from the sink and place it on the table. First, I need to figure out which actions are required. The model_selector tool returned PickUpAction, NavigateAction, and PlaceAction. That makes sense because the robot needs to move to the sink, pick up the cup, then navigate to the table and place it.\\n\\nNow, looking at the model_populator response. The NavigateAction has a target_location set to the table\\'s position. But wait, the original instruction says to go to the sink first. The NavigateAction here might be going directly to the table. Maybe the model_populator assumed the order, but the user\\'s instruction is to first go to the sink, pick up, then go to the table. However, the tool\\'s output might have combined the navigate steps. Or perhaps the NavigateAction here is for moving to the sink, and another NavigateAction for the table? Wait, the model_populator output shows only one NavigateAction. Hmm, maybe the tool inferred that after picking up, the robot needs to navigate to the table. But the initial position might be the sink. Wait, the robot\\'s initial position in the models is [-0.5, 0.0, 0.0], which might be the sink\\'s location. Then the NavigateAction target is the table at [1.2, 0.5, 0.0]. So the robot moves from sink to table, picks up the cup along the way? Wait, no. The PickUpAction is after the NavigateAction? Or are they in the correct order?\\n\\nWait, the user\\'s instruction is to pick up from the sink first. So the robot should first navigate to the sink, pick up the cup, then navigate to the table, and place it. But the model_populator output has a NavigateAction with target_location as the table. That might be incorrect. But maybe the model_populator assumed that the robot is already at the sink, so the NavigateAction is to go to the table. But the initial position in the models is the sink. So perhaps the NavigateAction is to move from the sink to the table. But the PickUpAction is in between. Wait, the model_populator output has three actions: PickUp, Navigate, Place. Wait, no, the order in the models is PickUp, Navigate, Place. That doesn\\'t make sense. The robot should first navigate to the sink, pick up, then navigate to the table, then place. But the model_populator output has the NavigateAction after PickUp, which would be incorrect. Wait, maybe the model_populator output is structured as a list of actions in the order they should be executed. Let me check the tool response again.\\n\\nLooking at the models returned by model_populator:\\n\\n1. PickUpAction: object_designator is cup, arm 0, grasp description.\\n2. NavigateAction: target_location is table\\'s position.\\n3. PlaceAction: object_designator is cup, target_location is table\\'s position, arm 0.\\n\\nWait, that\\'s not correct. The robot should first navigate to the sink, pick up the cup, then navigate to the table, then place. But according to the models, the NavigateAction is after PickUp, which would mean the robot is already at the sink, then navigates to the table. But the PickUpAction is before the NavigateAction, which might imply that the robot is at the sink, picks up, then moves to the table. Wait, maybe the NavigateAction here is to move from the sink to the table, but the PickUpAction is executed while moving? That doesn\\'t make sense. Maybe the model_populator combined the steps, assuming that the robot is already at the sink, so the NavigateAction is to go to the table, and the PickUpAction is executed before that. But that would be incorrect. Alternatively, perhaps the model_populator\\'s output is correct, and the order is: Navigate to sink (but that\\'s not in the model), pick up, then navigate to table, place. Wait, but the NavigateAction in the model is to the table. Maybe the initial position is the sink, so the NavigateAction is to move to the table. The PickUpAction is executed at the sink, then the NavigateAction moves to the table, then Place. But the model_populator\\'s output has the NavigateAction after the PickUpAction, which would be correct if the robot is already at the sink. Wait, maybe the initial position is the sink, so the robot doesn\\'t need to navigate there. Then the first action is PickUp, then Navigate to table, then Place. That would make sense. So the model_populator\\'s output is correct. The NavigateAction is to move from the sink (initial position) to the table. The PlaceAction is at the table. So the order is correct: PickUp, then Navigate to table, then Place. But the user\\'s instruction was to pick up from the sink first. So the model_populator\\'s output is correct. The initial position is the sink, so the robot doesn\\'t need to navigate there. Then pick up, then navigate to the table, then place. So the models are in the right order. Therefore, the answer should list the three actions in order: PickUp, Navigate, Place, with their respective parameters. The user might need to confirm if the initial position is indeed the sink, but based on the tool\\'s response, that\\'s how it\\'s structured.\\n</think>\\n\\nThe robot will execute the following sequence of actions to complete the task:\\n\\n1. **PickUpAction**  \\n   - **Object**: Cup  \\n   - **Arm**: 0 (left arm)  \\n   - **Grasp Configuration**: Front-facing, top-aligned, gripper rotation enabled.  \\n   - **Position**: Robot is already at the sink (initial position).  \\n\\n2. **NavigateAction**  \\n   - **Target Location**: Table at `[1.2, 0.5, 0.0]`  \\n   - **Orientation**: `[0.0, 0.0, 0.0, 1.0]` (facing forward).  \\n   - **Behavior**: Move from the sink to the table.  \\n\\n3. **PlaceAction**  \\n   - **Object**: Cup  \\n   - **Target Location**: Table at `[1.2, 0.5, 0.0]`  \\n   - **Arm**: 0 (left arm)  \\n   - **Position**: Robot arrives at the table after navigation.  \\n\\n**Summary**: The robot picks up the cup from the sink, navigates to the table, and places the cup there. The initial position is assumed to be the sink, so no explicit \"Navigate to sink\" action is required. The sequence ensures the cup is moved correctly.', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2025-05-21T14:37:42.347487662Z', 'done': True, 'done_reason': 'stop', 'total_duration': 20632279279, 'load_duration': 9333109, 'prompt_eval_count': 1350, 'prompt_eval_duration': 341802453, 'eval_count': 1418, 'eval_duration': 20248201155, 'model_name': 'qwen3:8b'}, id='run--a4a40db8-6a97-4496-b236-3a63c87c9e09-0', usage_metadata={'input_tokens': 1350, 'output_tokens': 1418, 'total_tokens': 2768})]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "instruction_for_populator = {\n",
    "        \"instruction\": \"pick up the cup from the sink\",\n",
    "        \"model_names\": [\"PickUpAction\"]\n",
    "    }"
   ],
   "id": "eaf67983d6da6c5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "res = model_selector.invoke(\"pick up the cup from the sink\")\n",
    "res"
   ],
   "id": "3262d035f8a0d146",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "res = model_populator.invoke({\"instruction\":\"pick up the cup from the sink\", \"model_names\": [\"PickUpAction\"]})\n",
    "res"
   ],
   "id": "bd26af86370bfc76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(res['populated_models'])",
   "id": "74cbee8b1420e0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Other Agents",
   "id": "884d46af51343620"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example: Complex Query Using Multiple Agents\n",
    "input_question = \"what is 2 times 2\"\n",
    "for s in graph.stream(\n",
    "    {\"messages\": [(\"user\", input_question)]},\n",
    "    subgraphs=True\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")\n"
   ],
   "id": "e54ed6f87aad0a36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example: Complex Query Using Multiple Agents\n",
    "input_question = \"what is the value of 2 + 10\"\n",
    "for s in graph.stream(\n",
    "    {\"messages\": [(\"user\", input_question)]},\n",
    "    subgraphs=True\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")\n"
   ],
   "id": "fc9e1a5c75b55215",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Example: Complex Query Using Multiple Agents\n",
    "# input_question = \"who is apple company founder\"\n",
    "# for s in graph.stream(\n",
    "#     {\"messages\": [(\"user\", input_question)]},\n",
    "#     subgraphs=True\n",
    "# ):\n",
    "#     print(s)\n",
    "#     print(\"----\")\n"
   ],
   "id": "b9980ce4b212f44a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f631ea35b1ae8d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Example: Complex Query Using Multiple Agents\n",
    "# input_question = \"who is the ceo of google and what is the framenet representation of apple\"\n",
    "# for s in graph.stream(\n",
    "#     {\"messages\": [(\"user\", input_question)]},\n",
    "#     subgraphs=True\n",
    "# ):\n",
    "#     print(s)\n",
    "#     print(\"----\")\n"
   ],
   "id": "7eb90ed0041629cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Example: Complex Query Using Multiple Agents\n",
    "# input_question = \"who is the ceo of google, what is the framenet representation of apple and how much is 2 times 10\"\n",
    "# for s in graph.stream(\n",
    "#     {\"messages\": [(\"user\", input_question)]},\n",
    "#     subgraphs=True\n",
    "# ):\n",
    "#     print(s)\n",
    "#     print(\"----\")\n"
   ],
   "id": "7274ab3752af7aaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3481b4af5579e10f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
